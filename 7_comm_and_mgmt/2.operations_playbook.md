# Operations Playbook

## Quick Reference
- [30-Second Checklists](#30-second-checklists) üöÄ
- [Emergency Responses](#emergency-responses) üÜò
- [Templates Library](#templates-library) üìã

## Table of Contents
1. [The Analysis Lifecycle](#the-analysis-lifecycle) - *Your core workflow*
2. [Stakeholder Management](#stakeholder-management) - *Getting requirements right*
3. [Technical Execution](#technical-execution) - *Doing the work correctly*
4. [Communication & Delivery](#communication--delivery) - *Presenting results*
5. [Experiment Management](#experiment-management) - *A/B testing end-to-end*
6. [Tool Mastery](#tool-mastery) - *Jira & Confluence like a pro*
7. [Cross-functional Collaboration](#cross-functional-collaboration) - *Working with DS/Product*
8. [Asynchronous Decision Making](#asynchronous-decision-making) - *How to avoid meetings overload*
9. [Stakeholder Simplification](#stakeholder-simplification) - *How to make sure your work is simple enough and drives decision - Also to help them better explain to you*
10. [Templates Library](#templates-ibrary)


---

## 30-Second Checklists

### üöÄ Starting Any Project
- [ ] Real business question identified (not just "pull data")
- [ ] Success criteria defined upfront
- [ ] Timeline confirmed with buffer built in
- [ ] Data availability validated
- [ ] Stakeholder expectations aligned

### üîç Before Sharing Results  
- [ ] Conclusions directly answer the business question
- [ ] Statistical significance checked
- [ ] Alternative explanations considered
- [ ] Actionable recommendations provided
- [ ] Follow-up questions anticipated

### üìù Jira Ticket Quality
- [ ] Business context in description
- [ ] Acceptance criteria specific and testable
- [ ] Story points estimated
- [ ] Labels and components set
- [ ] Dependencies identified

---

## The Analysis Lifecycle

### Phase 1: Problem Discovery (20% of time)
**Your goal: Transform vague requests into specific business questions**

#### The Stakeholder Interview Script
Start every project with these questions:
1. **"What business decision are you trying to make?"**
2. **"What would success look like? How would you measure it?"**
3. **"What's your hypothesis going in?"**
4. **"What would change your mind about this hypothesis?"**
5. **"When do you need this by, and what's driving that timeline?"**

#### Red Flags in Initial Requests
- "Can you just pull some data on..." ‚Üí *No clear decision to be made*
- "We need to understand customers better" ‚Üí *Too vague, no specific action*
- "Build a dashboard showing X" ‚Üí *Solution-oriented, not problem-oriented*
- "This should be quick" ‚Üí *Likely more complex than they think*

#### The Problem Definition Template
```
Business Question: [One clear sentence ending with ?]
Decision Maker: [Who will act on this analysis]  
Success Metrics: [Specific, measurable outcomes]
Current Hypothesis: [What stakeholder thinks is true]
Timeline: [Hard deadline + why]
Constraints: [What we can't change/access]
Out of Scope: [What we're NOT doing - be explicit]
```

### Phase 2: Project Planning (30% of time)
**Your goal: Set yourself up for success**

#### Analysis Planning Framework
```
Methodology Selection:
- Descriptive: What happened?
- Diagnostic: Why did it happen?  
- Predictive: What will happen?
- Prescriptive: What should we do?

Data Requirements:
- Primary sources: [List with recency/quality notes]
- Secondary validation: [How you'll double-check]
- Known limitations: [What's missing/imperfect]

Timeline Planning:
- Discovery: [X days]
- Data collection/cleaning: [Y days]  
- Analysis: [Z days]
- Validation/review: [A days]
- Communication: [B days]
- Buffer: [25-30% of total]
```

#### Risk Assessment Template
**Technical Risks:**
- Data quality/availability issues
- System performance constraints  
- Integration complexity

**Business Risks:**
- Changing requirements mid-stream
- Stakeholder misalignment
- Timeline compression pressure

**Mitigation Plans:**
- Plan B data sources identified upfront
- Regular stakeholder check-ins scheduled
- Interim deliverables planned

### Phase 3: Technical Execution (30% of time)
**Your goal: Deliver accurate, reliable insights**

#### The CRISP Analysis Framework
Apply this mental model to every analysis:

**C - Context**: What's the business situation?
- Market conditions, competitive landscape
- Recent changes in business/product
- Historical performance baselines

**R - Requirements**: What exactly are we measuring?
- Primary metrics and definitions
- Segmentation requirements
- Statistical rigor needed

**I - Investigation**: What does the data actually show?
- Descriptive statistics first
- Pattern identification
- Anomaly detection

**S - Synthesis**: What does this mean for the business?
- Business implications of findings
- Confidence levels and limitations
- Alternative explanations considered

**P - Prescription**: What should we do about it?
- Specific, actionable recommendations
- Expected impact quantification
- Implementation considerations

### Phase 4: Communication & Delivery (20% of time)
**Your goal: Drive business decisions**

#### The Three-Layer Communication Strategy

**Layer 1: The Headline (30 seconds)**
- One sentence: What did we find?
- One sentence: What should we do?
- One sentence: Why should we believe this?

**Layer 2: The Executive Summary (3 minutes)**
- Business context and question
- Key finding with confidence level
- Recommended action with rationale
- Expected impact and timeline

**Layer 3: The Deep Dive (30 minutes)**
- Detailed methodology
- Statistical evidence
- Sensitivity analysis
- Implementation planning

---

## Stakeholder Management

### Stakeholder Personality Types & How to Handle Them

#### The "Just Give Me Numbers" Executive
**Characteristics**: Wants quick answers, impatient with methodology
**Approach**: Lead with the business impact, methodology in appendix
**Language**: "This will increase revenue by X%" not "The p-value is 0.03"

#### The "Show Me Everything" Analyst
**Characteristics**: Wants to see all the data, questions methodology
**Approach**: Detailed technical appendix, peer review process
**Language**: Statistical rigor, confidence intervals, assumptions clearly stated

#### The "What About..." Product Manager  
**Characteristics**: Constantly thinks of edge cases and additional questions
**Approach**: Scope boundaries upfront, park follow-up questions for next phase
**Language**: "Great question - let's add that to the backlog for phase 2"

#### The "This Doesn't Look Right" Skeptic
**Characteristics**: Questions results that don't match expectations
**Approach**: Welcome the skepticism, validate together
**Language**: "Help me understand what seems off - let's dig into that"

### Managing Scope Creep

#### The Scope Creep Conversation Framework
1. **Acknowledge**: "That's a great additional question"
2. **Quantify**: "Adding that would take X more days because..."
3. **Trade-off**: "We could do that instead of Y, or push timeline to Z"
4. **Document**: "Let me update the Jira ticket to reflect this change"

#### Professional Pushback Templates
```
Timeline Pushback:
"I want to make sure we get this right for your decision. Based on similar analyses, here's what I'd expect: [timeline]. What's driving the urgency? Maybe we can find a middle ground."

Quality vs Speed:
"I can give you a directional answer by [date] and the full analysis by [date+]. Which would be more useful for your immediate needs?"

Resource Constraints:
"To do this properly, I'd need [resource/data/help]. The alternative is [reduced scope/accuracy]. What would work better for your decision?"
```

---

## Technical Execution

### Pre-Analysis Quality Gates

#### Data Quality Validation Checklist
**Before any analysis:**
- [ ] **Completeness**: Missing data patterns documented
- [ ] **Consistency**: Same metrics match across sources  
- [ ] **Recency**: Data freshness validated
- [ ] **Reasonableness**: Order of magnitude sense-checks
- [ ] **Granularity**: Right level of detail for question

#### Statistical Rigor Checklist
**For any statistical test:**
- [ ] **Sample size**: Power analysis completed
- [ ] **Assumptions**: Distribution assumptions validated
- [ ] **Multiple comparisons**: Bonferroni correction applied if needed
- [ ] **Practical significance**: Effect size meaningful for business
- [ ] **Confidence intervals**: Not just p-values reported

### Common Pitfalls & How to Avoid Them

#### The "Correlation = Causation" Trap
**Problem**: Stakeholders jump to causal conclusions
**Solution**: Always include "correlation doesn't imply causation" disclaimer
**Better**: Use causal inference methods (difference-in-difference, instrumental variables)

#### The "Survivorship Bias" Trap  
**Problem**: Only analyzing users/customers who stayed
**Solution**: Include churned users in cohort analyses
**Better**: Model attrition explicitly

#### The "Simpson's Paradox" Trap
**Problem**: Aggregated results hide segment-level patterns
**Solution**: Always analyze by key segments
**Better**: Use hierarchical modeling approaches

#### The "Multiple Comparisons" Trap
**Problem**: Testing many hypotheses inflates false positive rate
**Solution**: Apply Bonferroni correction or control FDR
**Better**: Pre-specify primary and secondary hypotheses

### Code Quality Standards

#### Documentation Requirements
```python
def analyze_campaign_performance(data, campaign_ids, date_range):
    """
    Analyzes marketing campaign performance metrics.
    
    Business Context:
    Used for monthly campaign reviews to identify top performers
    and optimization opportunities.
    
    Args:
        data (DataFrame): Campaign performance data from warehouse
        campaign_ids (list): Specific campaigns to analyze
        date_range (tuple): (start_date, end_date) for analysis period
        
    Returns:
        dict: {
            'summary_metrics': DataFrame with key KPIs,
            'segment_analysis': DataFrame with performance by segment,
            'recommendations': list of actionable insights
        }
        
    Assumptions:
    - Data is already deduplicated
    - Attribution model is last-click
    - Excludes test campaigns (campaign_type != 'test')
    
    Last Updated: 2024-01-15 by [your_name]
    Peer Review: [reviewer_name]
    """
```

#### Version Control Best Practices
```
Commit Message Format:
[TICKET-123] Brief description of change

Examples:
[ANAL-456] Add conversion rate analysis for email campaigns
[ANAL-457] Fix date filtering bug in cohort analysis  
[ANAL-458] Update bidding performance dashboard queries

Branch Naming:
feature/ANAL-123-campaign-analysis
bugfix/ANAL-124-date-filter-fix
```

---

## Communication & Delivery

### Audience-Specific Templates

#### Executive Summary (C-Suite/VP Level)
```markdown
# [Analysis Name] - Executive Summary

## Key Finding
[One sentence: What we discovered]

## Business Impact  
[Quantified impact: revenue, cost, efficiency]

## Recommended Action
[Specific action with owner and timeline]

## Confidence Level
[High/Medium/Low with brief rationale]

## Supporting Evidence
[2-3 bullet points of strongest evidence]

---
*Full analysis: [Link to detailed Confluence page]*
*Questions: [Your contact info]*
```

#### Technical Deep Dive (Data Science/Analyst Peers)
```markdown
# [Analysis Name] - Technical Documentation

## Problem Statement
[Business question and context]

## Methodology
### Data Sources
- [Source 1]: [Description, recency, limitations]
- [Source 2]: [Description, recency, limitations]

### Analytical Approach
- [Statistical method used and why]
- [Key assumptions made]
- [Validation techniques applied]

## Results
### Primary Finding
[Statistical significance, effect size, confidence intervals]

### Robustness Checks
[Sensitivity analysis results]

### Limitations
[What this analysis cannot tell us]

## Code & Reproducibility
- **Repository**: [Git link]
- **Key Functions**: [List main analysis functions]
- **Data Pipeline**: [How to recreate dataset]
- **Peer Review**: [Who reviewed, when]

## Appendix
[Detailed statistical output, additional charts]
```

### Presentation Best Practices

#### The "Pyramid Principle" for Slides
```
Slide 1: Answer (conclusion)
Slide 2: Supporting argument 1
Slide 3: Supporting argument 2  
Slide 4: Supporting argument 3
Slide 5: Next steps

NOT:
Slide 1: Background
Slide 2: Methodology
Slide 3: Data
Slide 4: Results
Slide 5: Conclusion (finally!)
```

#### Visualization Guidelines

**Chart Selection Quick Reference:**
- Trends over time ‚Üí Line chart
- Category comparisons ‚Üí Horizontal bar chart
- Distribution shape ‚Üí Histogram
- Relationship between variables ‚Üí Scatter plot
- Parts of whole ‚Üí Stacked bar (avoid pie charts)
- Performance vs target ‚Üí Bullet chart

**Design Principles:**
- Direct labeling beats legends
- Start y-axis at zero (unless strong reason not to)
- Use consistent color scheme across presentation
- Annotate key insights directly on chart
- Remove gridlines, borders, and other chart junk

---

## Experiment Management

### End-to-End Experiment Workflow

#### Phase 1: Experiment Design

**Hypothesis Development Template:**
```
We believe that [specific change]
will result in [specific outcome]
for [specific user segment]
because [theoretical reasoning/prior evidence].

We will measure this by [primary metric]
and expect to see [minimum detectable effect]
with [confidence level] certainty.
```

**Sample Size Calculation:**
```python
# Always document your power analysis
"""
Power Analysis for [Experiment Name]
- Baseline conversion rate: X%
- Minimum detectable effect: Y% (relative)  
- Statistical power: 80%
- Significance level: 5%
- Two-tailed test

Required sample size: N users per variant
Expected runtime: X weeks at current traffic
"""
```

#### Phase 2: Implementation & QA

**Pre-Launch Checklist:**
- [ ] **A/A Test**: Confirms no systematic differences
- [ ] **Sample Ratio Check**: Traffic split is as expected
- [ ] **Data Pipeline**: All metrics being tracked correctly
- [ ] **Feature Flag**: Toggle works in both directions
- [ ] **Guardrail Metrics**: Monitoring in place for key business metrics
- [ ] **Stakeholder Signoff**: All reviewers have approved

**Daily Monitoring Dashboard:**
```
Experiment Health Check:
- Sample ratio (should be ~50/50 for A/B test)
- Data freshness (latest timestamp)
- Primary metric trends (control vs treatment)
- Guardrail metric status (no significant degradation)
- Statistical power (current vs planned)
```

#### Phase 3: Analysis & Decision

**Statistical Analysis Framework:**
```
Primary Analysis:
- Statistical significance (p-value)
- Practical significance (effect size)
- Confidence interval for lift
- Statistical power achieved

Secondary Analysis:
- Segment-level results (mobile vs desktop, new vs returning)
- Time-based patterns (day of week, hour of day)
- Interaction effects with other experiments

Robustness Checks:
- Outlier sensitivity
- Different analysis windows
- Alternative statistical tests
```

**Decision Framework:**
```
Launch Criteria:
‚úÖ Primary metric statistically significant (p < 0.05)
‚úÖ Effect size practically meaningful (>X% lift)  
‚úÖ Guardrail metrics not degraded
‚úÖ Results consistent across key segments
‚úÖ No major implementation issues observed

Don't Launch Criteria:
‚ùå Primary metric not significant or negative
‚ùå Key guardrail metrics significantly degraded
‚ùå Results driven by outliers or data quality issues
‚ùå Significant technical issues during test period
```

### Experiment Documentation

**Confluence Experiment Page Template:**
```markdown
# Experiment: [Name] | Status: [Planning/Running/Complete] 

## TL;DR
**Result**: [Launch/Don't Launch/Iterate]
**Primary Metric Impact**: [X% lift, p=Y]  
**Business Impact**: [Revenue/conversion impact]

## Design
**Hypothesis**: [Formatted hypothesis from above]
**Test Period**: [Start] - [End] ([X weeks])
**Traffic**: [X% to treatment]
**Sample Size**: [N users, power analysis linked]

## Results
| Metric | Control | Treatment | Lift | P-value | Confidence |
|--------|---------|-----------|------|---------|------------|
| [Primary] | X% | Y% | +Z% | p=0.xx | [High/Med/Low] |
| [Secondary 1] | X% | Y% | +Z% | p=0.xx | [High/Med/Low] |

## Segment Analysis
[Key segment differences]

## Decision Rationale  
[Why we're launching/not launching]

## Next Steps
- [ ] [Action item] - [Owner] by [Date]
- [ ] [Related follow-up experiments]

---
**Analysis Code**: [Git link]
**Jira Ticket**: [Link]
**Analyst**: [Your name]
**Reviewed By**: [Peer reviewer]
```

---

## Tool Mastery

### Jira Best Practices

#### Ticket Templates by Type

**Analysis Story Template:**
```
Title: [ANALYSIS] [Business Area] - [Core Question]

Description:
üéØ Business Context:
[Stakeholder] needs to [make decision] because [business driver]

üìä Analysis Requirements:
- Primary question: [Specific question]
- Success metrics: [How we'll measure success]
- Timeline: [When needed and why]
- Constraints: [Any limitations]

‚úÖ Acceptance Criteria:
- [ ] Core business question answered with statistical backing
- [ ] Results validated by peer review
- [ ] Findings presented to stakeholder by [date]
- [ ] Recommendations documented with confidence levels  
- [ ] Follow-up actions identified and assigned

üîó Related Work:
- Epic: [Link to broader initiative]
- Dependencies: [What we need from others]
- Confluence Page: [Will be created]

Story Points: [1-13 based on complexity]
Labels: analysis, [business-area], [priority-level]
```

**Technical Task Template:**
```
Title: [TECH] [System] - [Specific Implementation]

Description:
üîß Technical Requirements:
- [Specific implementation details]
- [Performance/scalability needs]
- [Integration requirements]

‚úÖ Acceptance Criteria:
- [ ] [Specific, testable requirement 1]
- [ ] [Specific, testable requirement 2]  
- [ ] Unit tests written and passing
- [ ] Code reviewed and approved
- [ ] Documentation updated
- [ ] Production deployment successful

üîó Dependencies:
- Blocks: [What this enables]
- Blocked by: [Prerequisites]

Story Points: [Based on technical complexity]
Labels: technical, [system-name], [priority]
```

#### Sprint Management for Analysts

**Story Point Estimation for Analysis Work:**
- **1 point**: Simple data pull, metric validation (2-4 hours)
- **2 points**: Standard analysis, basic visualization (1 day)
- **3 points**: Complex analysis, statistical testing (2-3 days)
- **5 points**: Multi-week analysis, experiment design (1 week)
- **8 points**: Research project, new methodology (2 weeks)
- **13 points**: Break this down - too big for one sprint

**Daily Standup Template:**
```
Yesterday: 
- [TICKET-123]: Completed data validation, found quality issue with [source]
- [TICKET-124]: Peer review feedback incorporated

Today:
- [TICKET-123]: Implementing alternative approach, ETA still [date]
- [TICKET-125]: Starting stakeholder interviews

Blockers:
- Need access to [data source] for TICKET-126
- Waiting for product clarification on TICKET-127 requirements
```

### Confluence Organization

#### Space Structure for Analytics Team
```
üìÅ Marketing Analytics
‚îú‚îÄ‚îÄ üìÑ Team Home (dashboard with current work)
‚îú‚îÄ‚îÄ üìÅ Analysis Repository
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ 2024 Q1 Business Reviews
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ Experiment Results  
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ Ad-hoc Analyses
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ Methodology Documentation
‚îú‚îÄ‚îÄ üìÅ Data Dictionary & Sources
‚îú‚îÄ‚îÄ üìÅ Tools & Resources
‚îî‚îÄ‚îÄ üìÅ Meeting Notes
```

#### Page Labeling Strategy
**Consistent Labels for Easy Filtering:**
- **Content Type**: analysis, experiment, methodology, reference
- **Business Area**: bidding, attribution, growth, retention
- **Status**: in-progress, complete, archived
- **Priority**: urgent, high, medium, low
- **Team**: marketing-analytics, data-science, product

#### Smart Page Templates

**Weekly Business Review Template:**
```markdown
# Marketing Analytics - Week of [Date]

## Key Metrics Dashboard
[Embed live charts from your BI tool]

## This Week's Highlights
### üîç Analysis Completed
- **[Analysis Name]**: [Key finding] - [Link to full analysis]
- **[Analysis Name]**: [Key finding] - [Link to full analysis]

### üß™ Experiments Update  
- **[Experiment Name]**: [Status] - [Current results/next steps]
- **[Experiment Name]**: [Status] - [Current results/next steps]

### üö® Issues & Blockers
- [Issue description] - [Owner] - [Timeline]

## Next Week's Priorities
- [ ] [Priority 1] - [Owner]
- [ ] [Priority 2] - [Owner]

## Data Quality Notes
[Any ongoing data issues team should be aware of]

---
*Updated by [Your Name] | Next update: [Date]*
```

---

## Cross-functional Collaboration

### Working with Data Scientists

#### Strategy Planning Sessions
**Your Role as the Business Translator:**
```
Before the Meeting:
- Understand the business problem deeply
- Research existing solutions/approaches
- Prepare success criteria and constraints
- Identify key stakeholders and their needs

During the Meeting:
- Focus on business value, not just technical feasibility
- Ask about model interpretability requirements
- Discuss deployment and monitoring needs
- Align on evaluation metrics

After the Meeting:  
- Document decisions and assumptions
- Create Jira tickets for follow-up work
- Schedule regular check-ins
- Communicate updates to business stakeholders
```

**Technical Collaboration Best Practices:**
```
Model Development:
- Provide business context for feature engineering
- Help prioritize model requirements (accuracy vs speed vs interpretability)
- Validate model outputs against business logic
- Translate model results into business insights

Model Deployment:
- Define monitoring and alerting requirements
- Plan rollback procedures and success criteria
- Design A/B testing framework for model performance
- Create business impact measurement plan
```

### Working with Product Teams

#### Requirements Gathering for Analytics Features
```
Product Feature Request Template:

üéØ Business Objective:
What business problem are we solving? What's the expected impact?

üë• User Story:
As a [user type], I want [functionality] so that [benefit]

üìä Success Metrics:
How will we measure if this feature is successful?

üîß Technical Requirements:
- Data sources needed
- Performance/latency requirements  
- Integration points
- Reporting/monitoring needs

üìã Acceptance Criteria:
- [ ] [Specific, testable requirement]
- [ ] [Edge cases handled]
- [ ] [Performance benchmarks met]

üöÄ Go-to-Market:
- User adoption strategy
- Training/documentation needs
- Success measurement plan
```

#### Product Analytics Collaboration
```
Regular Touchpoints:
- Weekly product metrics review
- Monthly feature performance analysis  
- Quarterly strategic planning sessions
- Ad-hoc experiment result reviews

Your Value-Add:
- Statistical rigor for experiment design
- Business context for metric interpretation
- Cross-functional impact analysis
- Long-term trend identification and forecasting
```

---

## Emergency Responses

### Crisis Management Phrases

#### "This Analysis Looks Wrong"
```
1. Stay calm: "I appreciate you flagging this - let's dig in together"
2. Clarify: "What specifically seems off to you?"
3. Validate: "Let me walk through my methodology step by step"
4. Partner: "What would you expect to see instead?"
5. Document: "Let me make sure we capture any corrections needed"
```

#### "We Need This by Tomorrow"
```
1. Understand urgency: "Help me understand what's driving the timeline"
2. Scope options: "Here's what I can deliver by tomorrow vs next week"
3. Quality trade-offs: "For tomorrow, I can give you directional insights. For full statistical rigor, I'd need until [date]"
4. Deliver something: Always provide interim value, even if preliminary
```

#### "Can You Just Change This One Number?"
```
1. Understand impact: "Help me understand how that change affects the analysis"
2. Validate assumptions: "That would require changing [these assumptions] - is that correct?"
3. Assess implications: "Here's how that change would affect our other findings"
4. Document changes: "Let me update the documentation to reflect this new assumption"
```

#### "Why Is This Taking So Long?"
```
1. Acknowledge: "I understand the urgency - let me break down what's involved"
2. Educate: "For this type of analysis, here are the key steps: [list with timeframes]"
3. Options: "We could speed this up by [trade-offs], or I can deliver in phases"
4. Communicate progress: "Here's exactly where we are and what's left"
```

### Quick Wins When You're Behind

#### Day 1-3: Establish Credibility
- Review recent analyses in Confluence - understand team's standards
- Set up 1:1s with key stakeholders - understand their priorities
- Complete one small, high-visibility analysis perfectly
- Fix any obvious data quality issues you notice

#### Week 1: Build Relationships
- Volunteer for experiment reviews - show statistical knowledge
- Offer to peer review others' work - learn team standards
- Ask thoughtful questions in meetings - demonstrate business acumen
- Document something useful that was previously undocumented

#### Month 1: Add Strategic Value
- Identify a recurring analysis that could be automated
- Propose a new experiment or analysis that addresses key business question
- Present findings from your first major analysis
- Establish yourself as the go-to person for a specific business area

---

## Asynchronous Decision Making

### ‚úÖ Async Decision-Making for Mid-Process Check-Ins

This document defines a lightweight async framework to use when you're in the **middle of a project**, **analysis**, or **experiment**, and want to:

- Align on direction
- Raise an issue
- Suggest a pivot
- Get quick feedback without setting up a meeting

This is **not** for final proposals or experiment writeups‚Äîjust the ongoing moments that require visibility, alignment, and decisions.


### üîÅ When to Use This

Use this async format when:

- You're halfway through an analysis and see multiple directions forward.
- An unexpected data issue appears and affects timing or accuracy.
- A stakeholder gives unclear input and you want to scope options.
- A model or experiment isn't working as expected and needs a check-in.
- You found something new that might shift priorities.


### üß† Why It Works

- Saves everyone from unnecessary meetings.
- Makes your work **visible** while you‚Äôre still thinking.
- Creates a documented trail of decisions.
- Encourages quick feedback with a clear deadline.


### üí¨ Async Check-In Template

Use this in Slack, Notion, GitHub Issues, or Confluence:

```md
#### üìå What are we deciding or checking in on?

(A clear, one-sentence summary of the current situation or open question.)

#### üß† Context

(What you're working on, what stage you're at, what's changed.)

#### üîÄ Options / Current Thinking

- Option A: (brief + tradeoffs)
- Option B:
- Option C (if needed)

#### ‚úÖ My Recommendation

(Which path you suggest, and why.)

#### üë• Input Needed From

- @approver (who decides)
- @contributors (anyone with context)
- @informed (looped in for visibility)

#### üóìÔ∏è Feedback Needed By

(e.g. "Please comment or approve by Thursday 14:00 CET.")
```

### üîß Example Use Cases

| Scenario                         | How to Frame It                                                      |
|----------------------------------|----------------------------------------------------------------------|
| Mid-experiment results are unclear | ‚ÄúShould we extend the test, tweak the variant, or stop now?‚Äù         |
| Data pipeline is slower than expected | ‚ÄúOptions: wait, reduce scope, or build a fallback version.‚Äù           |
| Stakeholder request is vague     | ‚ÄúIs the real goal X, Y, or Z? Here's what we can do for each.‚Äù       |
| You're exploring a new idea      | ‚ÄúBefore I spend 2 more days on this, should we continue or pause?‚Äù   |


### üë• Decision Roles (Keep Simple)

| Role          | Description                                  |
|---------------|----------------------------------------------|
| **Driver**    | You. Framing and pushing the decision forward. |
| **Approver**  | Person who has final say (PM, Team Lead, etc.) |
| **Contributors** | People giving input or context               |
| **Informed**  | People to notify but not waiting on           |


*Copy ‚Üí Paste ‚Üí Adapt. This is your async ally for staying aligned without slowing down.*

---

## ü§ù Stakeholder Simplification Tools for Data Work

This doc summarizes proven tools, habits, and mental models to **simplify complex data work** for non-technical stakeholders‚Äîwithout losing nuance or accuracy.

Use these to:
- Clarify communication
- Align expectations
- Present insights that drive action
- Avoid misinterpretation or scope creep


### üóÇÔ∏è 1. The One-Slide Summary

> Can you explain your entire insight or recommendation in 1 slide or 1 paragraph?

#### Template:
```text
‚Ä¢ What we found  
‚Ä¢ Why it matters  
‚Ä¢ What we recommend  
```

- Use 1 chart max.
- Make the insight immediately scannable.
- Add a link to full notebook/report if needed.

---

### üì£ 2. "So What?" Layering (McKinsey-Style)

For every data point, ask:
> ‚ÄúSo what?‚Äù

**Example**:
- ‚ÄúChurn increased by 15%.‚Äù
- So what? ‚Üí ‚ÄúWe‚Äôre projected to lose ‚Ç¨200K in the next quarter.‚Äù
- So what? ‚Üí ‚ÄúWe should test retention incentives for segment X.‚Äù

**Why?** This pushes your work from ‚Äúreporting‚Äù to ‚Äústrategic recommendation.‚Äù

---

### üéØ 3. Decision-First Framing

> Stakeholders don‚Äôt want a chart‚Äîthey want to know what to do.

#### Framework:
```text
‚Ä¢ What‚Äôs the decision we‚Äôre making?
‚Ä¢ What does the data say?
‚Ä¢ What are the trade-offs?
‚Ä¢ What do we recommend?
```

**Benefits**:
- Focuses discussion
- Prevents rabbit holes
- Positions you as a thought partner

---

### üì¶ 4. Black Box Thinking (Simplified)

> When the internal workings are complex or invisible (e.g., ad auctions, recommender systems), focus on:

```text
Input ‚Üí [Black Box System] ‚Üí Output
```

### Stakeholder Communication:
- Emphasize what **we can control** (inputs)
- Show what **we can measure** (outputs)
- Acknowledge what we **don‚Äôt know** (the box)

Keeps the explanation grounded and actionable.

---

### üß† 5. 3-Layer Communication Model

Use layered communication to serve multiple audiences in one go:

| Layer        | Audience        | Format                        |
|--------------|------------------|-------------------------------|
| Headline     | Executives       | 1-sentence TL;DR              |
| Slide/chart  | PMs, stakeholders| Visual with annotations       |
| Appendix     | Technical peers  | Link to notebook, full query  |

---

### üß∞ 6. Simplification Toolkit

| Tool                         | Use Case                                          |
|------------------------------|---------------------------------------------------|
| **Metric Glossary**          | Align on terminology (CLV, ROAS, attribution)     |
| **If-this-then-that trees**  | Show logical outcomes of scenarios                |
| **Standard analysis template** | Keep insights consistent across stakeholders     |
| **Slack/Notion async format** | Use TL;DR / Context / What I need from you        |
| **Quick sketch diagrams**    | Rough whiteboards > long paragraphs               |

---

### üß™ 7. Run a Pre-Mortem

Before kicking off a complex analysis, ask:

> ‚ÄúImagine this work is misunderstood, ignored, or delayed. Why?‚Äù

This reveals:
- Stakeholders who need early context
- Potential points of confusion
- Missing clarity in goals or ownership

Build this into your project kickoff habit.

---

### üßæ Quick Reference Summary

| Tool / Habit                  | Purpose                          |
|------------------------------|----------------------------------|
| ‚úÖ One-Slide Summary          | Boil it down to insight + action |
| üéØ Decision-First Framing     | Make decisions the focus         |
| üß† ‚ÄúSo What?‚Äù Layering        | Tie insight to business impact   |
| üì¶ Black Box Model            | Explain complex systems simply   |
| üìÑ 3-Layer Format             | Serve execs + peers in one deck  |
| üß∞ Glossaries & Logic Trees   | Prevent misalignment             |
| üîç Pre-Mortem Thinking        | Avoid stakeholder surprises      |


*Keep this doc handy for every stakeholder-facing project. The better you simplify, the more likely your work drives action.*


---

## Templates Library

### Email Templates

#### Stakeholder Check-in
```
Subject: [Analysis Name] - Progress Update & Quick Question

Hi [Name],

Quick update on the [analysis name] we discussed:

‚úÖ Completed: [Key milestone]
üìä Current status: [Brief progress summary]  
‚è∞ Timeline: Still on track for [date]

One quick question: [Specific question that could affect analysis]

I'll have preliminary findings ready for review by [date]. Does [day/time] work for a 15-minute sync to walk through initial results?

Best,
[Your name]
```

#### Results Delivery
```
Subject: [Analysis Name] - Key Findings & Recommendations

Hi [Team],

Completed the [analysis name]. Here are the key takeaways:

üéØ **Bottom Line**: [One sentence summary]

üìä **Key Finding**: [Primary insight with confidence level]

üöÄ **Recommendation**: [Specific action with expected impact]

üìã **Next Steps**: 
- [Action item] - [Owner] by [Date]
- [Action item] - [Owner] by [Date]

Full analysis documented in Confluence: [Link]
Happy to discuss - let me know if you have questions.

Best,
[Your name]
```

### Meeting Templates

#### Analysis Kickoff Meeting Agenda
```
üìã [Analysis Name] - Kickoff Meeting
üïê [Duration: 30 minutes]

Attendees: [List with roles]

Agenda:
1. Business Context (5 min)
   - What decision are we trying to make?
   - What's the business impact?

2. Requirements Alignment (10 min)  
   - Success criteria confirmation
   - Key metrics and definitions
   - Timeline and constraints

3. Methodology Discussion (10 min)
   - Proposed approach
   - Data sources and limitations
   - Statistical rigor requirements

4. Next Steps (5 min)
   - Deliverables and timeline
   - Check-in schedule
   - Contact preferences

Pre-read: [Link to background materials]
```

#### Results Presentation Agenda
```
üìä [Analysis Name] - Results Review
üïê [Duration: 45 minutes]

Attendees: [Decision makers and stakeholders]

Agenda:
1. Executive Summary (5 min)
   - Key finding and recommendation
   - Confidence level and limitations

2. Supporting Analysis (15 min)
   - Methodology overview
   - Key results and insights
   - Statistical significance

3. Business Implications (15 min)
   - Impact quantification
   - Implementation considerations
   - Risk assessment

4. Next Steps & Discussion (10 min)
   - Recommended actions
   - Timeline and ownership
   - Follow-up questions

Materials: [Link to Confluence page with full analysis]
```

---

*This playbook is a living document. Update it as you learn new techniques and face new situations. The goal is to make every analysis better than the last.*
